{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tag=V_curword=love': 1, 'tag=V_biasterm': 1}\n",
      "{'tag=A_biasterm': 1, 'tag=A_curword=Happy': 1}\n",
      "{'tag=A_biasterm': 1, 'tag=A_curword=Happy': 1, 'tag=N_curword=Abe': 1, 'lasttag=A_curtag=N': 1, 'tag=N_curword=president': 1, 'lasttag=N_curtag=N': 2, 'tag=N_biasterm': 3, 'tag=N_curword=Lincoln': 1}\n",
      "{('D', '^'): 0.0, ('M', '@'): 0.0, ('^', '&'): 0.0, ('N', '#'): 0.0, ('G', 'G'): 0.0, ('X', 'D'): 0.0, ('#', 'U'): 0.0, ('E', 'M'): 0.0, (',', 'U'): 0.0, ('S', '!'): 0.0, ('@', 'X'): 0.0, ('A', 'N'): 0.0, ('T', '@'): 0.0, ('M', '&'): 0.0, ('A', 'Y'): 0.0, ('R', '#'): 0.0, ('O', '!'): 0.0, ('Y', 'V'): 0.0, ('O', 'O'): 0.0, ('V', 'X'): 0.0, ('G', 'E'): 0.0, ('E', 'D'): 0.0, ('&', '&'): 0.0, ('!', '&'): 0.0, ('R', '$'): 0.0, ('O', 'N'): 0.0, ('X', 'M'): 0.0, ('Z', '$'): 0.0, ('Z', '^'): 0.0, ('Y', 'E'): 0.0, ('G', 'D'): 0.0, ('R', '^'): 0.0, ('T', 'N'): 0.0, ('#', '@'): 0.0, ('Y', 'X'): 0.0, ('O', 'M'): 0.0, ('V', ','): 0.0, ('S', 'S'): 0.0, ('N', 'T'): 0.0, ('X', '^'): 0.0, ('R', '&'): 0.0, ('$', 'S'): 0.0, ('N', '^'): 0.0, ('O', 'L'): 0.0, ('Z', 'P'): 0.0, ('@', 'U'): 0.0, ('L', '#'): 0.0, ('#', '^'): 0.0, ('$', '&'): 0.0, ('D', 'S'): 0.0, ('D', 'X'): 0.0, ('V', 'T'): 0.0, ('P', 'V'): 0.0, ('P', 'O'): 0.0, ('E', '!'): 0.0, ('P', 'D'): 0.0, ('D', '$'): 0.0, ('S', '$'): 0.0, ('N', 'O'): 0.0, ('A', 'M'): 0.0, ('L', '!'): 0.0, ('U', 'V'): 0.0, (',', 'L'): 0.0, ('Z', 'S'): 0.0, ('S', 'O'): 0.0, ('X', 'G'): 0.0, ('&', 'X'): 0.0, ('X', 'L'): 0.0, ('G', 'Z'): 0.0, ('E', '#'): 0.0, (',', 'M'): 0.0, ('A', 'O'): 0.0, ('@', 'P'): 0.0, ('U', 'D'): 0.0, ('M', '^'): 0.0, ('^', 'V'): 0.0, ('@', '&'): 0.0, ('#', 'Z'): 0.0, ('Y', '^'): 0.0, ('^', '@'): 0.0, ('V', 'P'): 0.0, ('!', ','): 0.0, ('!', 'S'): 0.0, ('E', 'L'): 0.0, ('R', ','): 0.0, ('O', 'V'): 0.0, ('V', 'N'): 0.0, ('Z', 'V'): 0.0, ('G', '&'): 0.0, ('T', 'A'): 0.0, ('T', 'V'): 0.0, (',', 'Z'): 0.0, ('#', 'X'): 0.0, ('O', 'U'): 0.0, ('&', 'T'): 0.0, ('X', 'P'): 0.0, ('E', 'N'): 0.0, ('M', 'A'): 0.0, ('G', '^'): 0.0, ('N', '&'): 0.0, ('O', 'T'): 0.0, ('#', '#'): 0.0, ('L', '^'): 0.0, ('V', 'S'): 0.0, ('A', 'Z'): 0.0, ('!', '!'): 0.0, ('M', 'Z'): 0.0, ('A', 'U'): 0.0, ('#', 'V'): 0.0, ('P', 'X'): 0.0, ('D', '@'): 0.0, ('P', 'N'): 0.0, ('P', 'G'): 0.0, ('Y', 'S'): 0.0, ('D', ','): 0.0, ('#', '!'): 0.0, ('V', '$'): 0.0, ('S', 'Y'): 0.0, ('#', 'T'): 0.0, ('^', '$'): 0.0, (',', 'D'): 0.0, ('D', 'Y'): 0.0, ('E', 'O'): 0.0, ('D', 'N'): 0.0, ('X', 'O'): 0.0, ('Y', 'U'): 0.0, ('&', 'P'): 0.0, ('X', 'T'): 0.0, ('&', '#'): 0.0, ('G', 'R'): 0.0, ('$', 'G'): 0.0, (',', 'E'): 0.0, ('@', 'Z'): 0.0, ('A', 'L'): 0.0, ('L', 'Z'): 0.0, ('@', 'S'): 0.0, ('A', 'G'): 0.0, ('Y', 'P'): 0.0, ('A', '^'): 0.0, ('T', 'P'): 0.0, ('M', 'V'): 0.0, ('^', '^'): 0.0, ('#', 'R'): 0.0, ('V', '^'): 0.0, ('Y', '&'): 0.0, ('E', 'S'): 0.0, ('&', 'Y'): 0.0, ('P', 'U'): 0.0, ('!', '$'): 0.0, ('O', '^'): 0.0, ('L', 'X'): 0.0, ('N', 'A'): 0.0, ('U', 'A'): 0.0, ('T', '^'): 0.0, ('$', 'T'): 0.0, (',', 'R'): 0.0, ('S', 'U'): 0.0, ('#', 'P'): 0.0, ('X', 'S'): 0.0, ('&', 'L'): 0.0, ('X', 'X'): 0.0, ('Z', 'L'): 0.0, ('P', '#'): 0.0, ('Y', '@'): 0.0, ('$', 'U'): 0.0, ('Y', 'Y'): 0.0, (',', 'S'): 0.0, ('@', 'A'): 0.0, ('T', '&'): 0.0, ('A', '@'): 0.0, ('L', 'V'): 0.0, ('U', ','): 0.0, ('A', 'R'): 0.0, ('Z', 'M'): 0.0, ('M', 'R'): 0.0, ('V', 'Z'): 0.0, ('P', 'P'): 0.0, ('&', 'U'): 0.0, ('V', '#'): 0.0, ('S', '&'): 0.0, (',', 'V'): 0.0, ('L', 'T'): 0.0, ('Z', 'O'): 0.0, ('$', 'P'): 0.0, ('X', 'A'): 0.0, ('^', ','): 0.0, ('&', '^'): 0.0, ('D', 'A'): 0.0, ('E', 'Y'): 0.0, ('^', 'X'): 0.0, ('U', '^'): 0.0, ('^', 'E'): 0.0, ('@', 'L'): 0.0, ('$', 'O'): 0.0, ('@', 'R'): 0.0, ('A', 'D'): 0.0, ('L', 'R'): 0.0, ('^', 'T'): 0.0, ('T', 'S'): 0.0, ('A', 'V'): 0.0, ('T', 'X'): 0.0, ('M', 'N'): 0.0, ('X', 'E'): 0.0, ('S', 'X'): 0.0, ('E', 'P'): 0.0, ('V', 'V'): 0.0, ('X', '&'): 0.0, ('S', 'G'): 0.0, ('P', 'M'): 0.0, ('X', 'R'): 0.0, ('!', 'N'): 0.0, ('M', ','): 0.0, ('L', 'P'): 0.0, ('N', 'L'): 0.0, ('Y', 'L'): 0.0, ('O', 'A'): 0.0, ('S', 'M'): 0.0, ('E', 'R'): 0.0, ('&', 'Z'): 0.0, ('O', '@'): 0.0, ('&', 'D'): 0.0, ('Z', 'D'): 0.0, ('U', '$'): 0.0, ('N', '$'): 0.0, ('G', 'N'): 0.0, ('@', 'Y'): 0.0, ('L', 'Y'): 0.0, ('M', '!'): 0.0, ('A', 'X'): 0.0, ('L', 'N'): 0.0, ('U', '@'): 0.0, ('A', 'S'): 0.0, ('Z', 'E'): 0.0, ('D', 'T'): 0.0, ('S', 'T'): 0.0, ('P', 'Z'): 0.0, ('P', 'S'): 0.0, ('V', 'R'): 0.0, ('Y', '!'): 0.0, ('&', 'M'): 0.0, ('#', ','): 0.0, ('Y', '#'): 0.0, ('^', 'O'): 0.0, (',', '!'): 0.0, ('M', '#'): 0.0, ('G', ','): 0.0, (',', 'X'): 0.0, ('Z', 'G'): 0.0, ('D', 'R'): 0.0, ('$', 'X'): 0.0, ('&', 'V'): 0.0, ('E', 'A'): 0.0, (',', 'Y'): 0.0, ('&', '@'): 0.0, ('@', 'O'): 0.0, ('$', 'Y'): 0.0, ('^', 'M'): 0.0, ('@', 'D'): 0.0, ('T', 'D'): 0.0, ('L', 'U'): 0.0, ('#', 'O'): 0.0, ('!', '#'): 0.0, ('@', '#'): 0.0, ('Y', 'R'): 0.0, ('^', 'N'): 0.0, ('S', 'P'): 0.0, ('#', 'G'): 0.0, ('E', 'X'): 0.0, ('^', 'P'): 0.0, ('P', 'E'): 0.0, ('Z', 'Z'): 0.0, ('T', '#'): 0.0, ('M', '$'): 0.0, ('U', 'T'): 0.0, ('L', 'S'): 0.0, ('#', 'M'): 0.0, ('$', 'V'): 0.0, ('Y', 'T'): 0.0, ('V', '&'): 0.0, ('T', 'Y'): 0.0, ('Z', '!'): 0.0, ('^', 'D'): 0.0, ('$', 'D'): 0.0, ('S', 'E'): 0.0, ('E', 'Z'): 0.0, ('N', '@'): 0.0, ('&', 'R'): 0.0, ('V', 'M'): 0.0, ('N', 'R'): 0.0, ('#', '&'): 0.0, ('$', 'E'): 0.0, ('N', ','): 0.0, ('T', '!'): 0.0, ('M', 'Y'): 0.0, ('A', 'P'): 0.0, ('S', 'N'): 0.0, ('G', '#'): 0.0, ('S', '^'): 0.0, ('U', 'S'): 0.0, ('Z', '#'): 0.0, ('S', 'L'): 0.0, ('P', 'R'): 0.0, ('Y', 'G'): 0.0, ('P', '@'): 0.0, ('&', 'E'): 0.0, ('#', '$'): 0.0, ('^', '!'): 0.0, ('!', 'Z'): 0.0, ('A', '!'): 0.0, ('L', 'O'): 0.0, ('L', 'D'): 0.0, ('$', 'R'): 0.0, (',', 'P'): 0.0, ('D', 'U'): 0.0, ('D', 'Z'): 0.0, ('$', '@'): 0.0, ('S', 'A'): 0.0, ('&', 'N'): 0.0, ('P', '!'): 0.0, ('D', '&'): 0.0, ('O', '&'): 0.0, ('@', 'N'): 0.0, ('Z', '&'): 0.0, ('@', 'G'): 0.0, ('$', 'A'): 0.0, ('T', 'G'): 0.0, ('T', 'L'): 0.0, ('L', 'M'): 0.0, ('M', 'U'): 0.0, ('A', 'T'): 0.0, ('Y', 'Z'): 0.0, ('O', 'S'): 0.0, ('S', 'Z'): 0.0, ('U', '&'): 0.0, ('!', 'M'): 0.0, ('P', 'Y'): 0.0, (',', '&'): 0.0, ('^', 'Z'): 0.0, ('E', '@'): 0.0, ('&', 'A'): 0.0, ('G', 'L'): 0.0, ('O', '$'): 0.0, ('O', 'R'): 0.0, ('!', 'L'): 0.0, ('Z', 'R'): 0.0, ('T', 'E'): 0.0, ('!', '^'): 0.0, (',', '^'): 0.0, ('#', 'E'): 0.0, ('L', '@'): 0.0, ('$', '^'): 0.0, ('N', 'Y'): 0.0, ('U', 'Y'): 0.0, ('$', 'L'): 0.0, ('X', 'U'): 0.0, ('Y', 'M'): 0.0, ('P', ','): 0.0, ('R', '@'): 0.0, ('V', 'E'): 0.0, ('N', 'Z'): 0.0, ('O', 'P'): 0.0, ('Z', 'T'): 0.0, ('Y', 'D'): 0.0, ('$', 'M'): 0.0, ('G', 'Y'): 0.0, ('R', 'A'): 0.0, ('U', 'P'): 0.0, ('L', 'L'): 0.0, ('S', 'V'): 0.0, ('!', 'A'): 0.0, ('Z', 'U'): 0.0, ('D', 'D'): 0.0, ('X', '$'): 0.0, ('S', 'D'): 0.0, ('&', ','): 0.0, ('&', 'S'): 0.0, ('Y', 'O'): 0.0, ('G', 'X'): 0.0, ('$', '#'): 0.0, ('!', '@'): 0.0, ('!', 'R'): 0.0, ('M', 'X'): 0.0, ('L', 'G'): 0.0, ('M', 'S'): 0.0, ('U', 'R'): 0.0, ('$', 'Z'): 0.0, ('E', 'U'): 0.0, ('Y', '$'): 0.0, ('E', '$'): 0.0, ('X', 'Y'): 0.0, ('R', 'D'): 0.0, ('V', 'A'): 0.0, ('G', '$'): 0.0, ('T', 'O'): 0.0, ('@', 'T'): 0.0, ('T', 'T'): 0.0, ('^', 'R'): 0.0, ('L', 'E'): 0.0, ('M', 'M'): 0.0, ('R', 'E'): 0.0, ('G', '!'): 0.0, ('^', 'L'): 0.0, ('S', 'R'): 0.0, ('!', 'E'): 0.0, ('X', '#'): 0.0, ('S', '@'): 0.0, ('E', '&'): 0.0, ('Y', 'A'): 0.0, ('O', ','): 0.0, ('O', 'Z'): 0.0, ('!', 'D'): 0.0, ('N', 'E'): 0.0, ('T', 'M'): 0.0, ('U', 'E'): 0.0, ('!', 'V'): 0.0, ('T', 'R'): 0.0, ('M', 'T'): 0.0, ('G', 'S'): 0.0, ('M', 'O'): 0.0, ('R', 'G'): 0.0, ('N', '!'): 0.0, ('O', 'Y'): 0.0, ('^', 'U'): 0.0, ('N', 'P'): 0.0, ('P', '$'): 0.0, ('^', 'S'): 0.0, ('@', 'M'): 0.0, ('S', '#'): 0.0, ('O', 'X'): 0.0, ('U', 'G'): 0.0, ('G', 'V'): 0.0, ('@', '!'): 0.0, ('V', 'L'): 0.0, ('L', 'A'): 0.0, ('U', 'X'): 0.0, (',', ','): 0.0, ('D', 'G'): 0.0, ('!', 'Y'): 0.0, ('D', 'L'): 0.0, ('X', ','): 0.0, ('&', '$'): 0.0, ('G', 'P'): 0.0, ('D', '#'): 0.0, ('V', '!'): 0.0, ('!', 'X'): 0.0, ('A', '&'): 0.0, ('M', 'P'): 0.0, ('N', 'S'): 0.0, ('#', 'Y'): 0.0, ('U', 'Z'): 0.0, ('X', 'Z'): 0.0, (',', '@'): 0.0, ('D', 'E'): 0.0, ('E', ','): 0.0, ('V', 'O'): 0.0, ('^', 'Y'): 0.0, ('D', '!'): 0.0, ('R', 'L'): 0.0, ('V', 'Y'): 0.0, (',', 'A'): 0.0, ('@', '^'): 0.0, ('L', ','): 0.0, ('@', ','): 0.0, ('G', 'U'): 0.0, ('M', 'E'): 0.0, ('R', 'M'): 0.0, ('E', 'T'): 0.0, ('Z', 'N'): 0.0, ('&', 'G'): 0.0, ('G', 'T'): 0.0, ('U', '!'): 0.0, ('R', 'N'): 0.0, ('T', '$'): 0.0, ('^', '#'): 0.0, ('A', '#'): 0.0, ('N', 'M'): 0.0, ('T', 'U'): 0.0, ('U', 'M'): 0.0, ('T', 'Z'): 0.0, ('M', 'L'): 0.0, (',', 'N'): 0.0, ('M', 'G'): 0.0, ('R', 'O'): 0.0, ('E', 'V'): 0.0, ('$', 'N'): 0.0, ('Y', ','): 0.0, ('X', 'N'): 0.0, ('N', 'N'): 0.0, ('Z', '@'): 0.0, ('N', 'X'): 0.0, (',', 'O'): 0.0, ('U', '#'): 0.0, ('R', 'P'): 0.0, ('@', 'E'): 0.0, ('$', ','): 0.0, ('V', 'U'): 0.0, ('#', 'N'): 0.0, ('U', 'O'): 0.0, ('Z', 'A'): 0.0, ('V', 'D'): 0.0, ('$', '$'): 0.0, ('#', 'S'): 0.0, ('!', 'O'): 0.0, ('^', 'G'): 0.0, (',', '$'): 0.0, ('D', 'O'): 0.0, ('P', 'T'): 0.0, ('R', 'R'): 0.0, ('A', ','): 0.0, ('!', 'P'): 0.0, ('#', 'L'): 0.0, ('L', '&'): 0.0, ('D', 'V'): 0.0, ('G', 'O'): 0.0, ('R', 'S'): 0.0, ('D', 'M'): 0.0, ('E', 'E'): 0.0, ('V', 'G'): 0.0, ('^', 'A'): 0.0, ('@', '@'): 0.0, ('R', 'T'): 0.0, ('A', 'A'): 0.0, ('@', 'V'): 0.0, ('L', '$'): 0.0, ('Y', 'N'): 0.0, ('O', 'G'): 0.0, ('@', '$'): 0.0, ('V', '@'): 0.0, ('G', 'M'): 0.0, ('R', 'U'): 0.0, ('E', 'G'): 0.0, ('!', 'U'): 0.0, ('P', 'A'): 0.0, ('Z', ','): 0.0, ('$', '!'): 0.0, ('R', 'V'): 0.0, ('T', ','): 0.0, ('!', 'T'): 0.0, ('N', 'U'): 0.0, ('O', 'E'): 0.0, ('U', 'U'): 0.0, ('M', 'D'): 0.0, ('X', '@'): 0.0, (',', '#'): 0.0, ('E', '^'): 0.0, ('N', 'D'): 0.0, ('X', 'V'): 0.0, ('&', '!'): 0.0, ('N', 'V'): 0.0, ('O', 'D'): 0.0, ('P', '&'): 0.0, ('Z', 'X'): 0.0, ('&', 'O'): 0.0, (',', 'G'): 0.0, ('R', 'X'): 0.0, ('A', 'E'): 0.0, ('U', 'L'): 0.0, ('Z', 'Y'): 0.0, ('D', 'P'): 0.0, ('G', 'A'): 0.0, ('P', '^'): 0.0, ('R', 'Y'): 0.0, ('!', 'G'): 0.0, ('X', '!'): 0.0, ('P', 'L'): 0.0, ('U', 'N'): 0.0, ('S', ','): 0.0, ('#', 'A'): 0.0, ('G', '@'): 0.0, ('R', 'Z'): 0.0, ('N', 'G'): 0.0, ('A', '$'): 0.0, ('R', '!'): 0.0, ('#', 'D'): 0.0, ('O', '#'): 0.0, (',', 'T'): 0.0}\n",
      "[{'!': 0.0, '#': 0.0, '$': 0.0, '&': 0.0, ',': 0.0, 'A': 0.0, '@': 0.0, 'E': 0.0, 'D': 0.0, 'G': 0.0, 'M': 0.0, 'L': 0.0, 'O': 0.0, 'N': 0.0, 'P': 0.0, 'S': 0.0, 'R': 0.0, 'U': 0.0, 'T': 0.0, 'V': 0.0, 'Y': 0.0, 'X': 0.0, 'Z': 0.0, '^': 0.0}, {'!': 0.0, '#': 0.0, '$': 0.0, '&': 0.0, ',': 0.0, 'A': 0.0, '@': 0.0, 'E': 0.0, 'D': 0.0, 'G': 0.0, 'M': 0.0, 'L': 0.0, 'O': 0.0, 'N': 0.0, 'P': 0.0, 'S': 0.0, 'R': 0.0, 'U': 0.0, 'T': 0.0, 'V': 0.0, 'Y': 0.0, 'X': 0.0, 'Z': 0.0, '^': 0.0}]\n",
      "['!', '!']\n",
      "Training iteration 0\n",
      "TR  RAW EVAL: 8523/14619 = 0.5830 accuracy\n",
      "DEV RAW EVAL: 2556/4823 = 0.5300 accuracy\n",
      "DEV AVG EVAL: 2986/4823 = 0.6191 accuracy\n",
      "Training iteration 1\n",
      "TR  RAW EVAL: 10643/14619 = 0.7280 accuracy\n",
      "DEV RAW EVAL: 2956/4823 = 0.6129 accuracy\n",
      "DEV AVG EVAL: 3170/4823 = 0.6573 accuracy\n",
      "Training iteration 2\n",
      "TR  RAW EVAL: 10148/14619 = 0.6942 accuracy\n",
      "DEV RAW EVAL: 2692/4823 = 0.5582 accuracy\n",
      "DEV AVG EVAL: 3281/4823 = 0.6803 accuracy\n",
      "Training iteration 3\n",
      "TR  RAW EVAL: 11681/14619 = 0.7990 accuracy\n",
      "DEV RAW EVAL: 3150/4823 = 0.6531 accuracy\n",
      "DEV AVG EVAL: 3311/4823 = 0.6865 accuracy\n",
      "Training iteration 4\n",
      "TR  RAW EVAL: 11722/14619 = 0.8018 accuracy\n",
      "DEV RAW EVAL: 3028/4823 = 0.6278 accuracy\n",
      "DEV AVG EVAL: 3322/4823 = 0.6888 accuracy\n",
      "Training iteration 5\n",
      "TR  RAW EVAL: 11799/14619 = 0.8071 accuracy\n",
      "DEV RAW EVAL: 3003/4823 = 0.6226 accuracy\n",
      "DEV AVG EVAL: 3333/4823 = 0.6911 accuracy\n",
      "Training iteration 6\n",
      "TR  RAW EVAL: 10765/14619 = 0.7364 accuracy\n",
      "DEV RAW EVAL: 2839/4823 = 0.5886 accuracy\n",
      "DEV AVG EVAL: 3340/4823 = 0.6925 accuracy\n",
      "Training iteration 7\n",
      "TR  RAW EVAL: 12355/14619 = 0.8451 accuracy\n",
      "DEV RAW EVAL: 3076/4823 = 0.6378 accuracy\n",
      "DEV AVG EVAL: 3346/4823 = 0.6938 accuracy\n",
      "Training iteration 8\n",
      "TR  RAW EVAL: 11224/14619 = 0.7678 accuracy\n",
      "DEV RAW EVAL: 2947/4823 = 0.6110 accuracy\n",
      "DEV AVG EVAL: 3349/4823 = 0.6944 accuracy\n",
      "Training iteration 9\n",
      "TR  RAW EVAL: 12581/14619 = 0.8606 accuracy\n",
      "DEV RAW EVAL: 3232/4823 = 0.6701 accuracy\n",
      "DEV AVG EVAL: 3341/4823 = 0.6927 accuracy\n",
      "Learned weights for 24359 features from 1000 examples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import sys,re,random\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from vit import viterbi\n",
    "\n",
    "##########################\n",
    "# Stuff you will use\n",
    "\n",
    "import vit  # your vit.py from part 1\n",
    "OUTPUT_VOCAB = set(\"\"\" ! # $ & , @ A D E G L M N O P R S T U V X Y Z ^ \"\"\".split())\n",
    "\n",
    "##########################\n",
    "# Utilities\n",
    "\n",
    "def dict_subtract(vec1, vec2):\n",
    "    \"\"\"treat vec1 and vec2 as dict representations of sparse vectors\"\"\"\n",
    "    out = defaultdict(float)\n",
    "    out.update(vec1)\n",
    "    for k in vec2: out[k] -= vec2[k]\n",
    "    return dict(out)\n",
    "\n",
    "def dict_argmax(dct):\n",
    "    \"\"\"Return the key whose value is largest. In other words: argmax_k dct[k]\"\"\"\n",
    "    return max(dct.iterkeys(), key=lambda k: dct[k])\n",
    "\n",
    "def dict_dotprod(d1, d2):\n",
    "    \"\"\"Return the dot product (aka inner product) of two vectors, where each is\n",
    "    represented as a dictionary of {index: weight} pairs, where indexes are any\n",
    "    keys, potentially strings.  If a key does not exist in a dictionary, its\n",
    "    value is assumed to be zero.\"\"\"\n",
    "    smaller = d1 if len(d1)<len(d2) else d2  # BUGFIXED 20151012\n",
    "    total = 0\n",
    "    for key in smaller.iterkeys():\n",
    "        total += d1.get(key,0) * d2.get(key,0)\n",
    "    return total\n",
    "\n",
    "def read_tagging_file(filename):\n",
    "    \"\"\"Returns list of sentences from a two-column formatted file.\n",
    "    Each returned sentence is the pair (tokens, tags) where each of those is a\n",
    "    list of strings.\n",
    "    \"\"\"\n",
    "    sentences = open(filename).read().strip().split(\"\\n\\n\")\n",
    "    ret = []\n",
    "    for sent in sentences:\n",
    "        lines = sent.split(\"\\n\")\n",
    "        pairs = [L.split(\"\\t\") for L in lines]\n",
    "        tokens = [tok for tok,tag in pairs]\n",
    "        tags = [tag for tok,tag in pairs]\n",
    "        ret.append( (tokens,tags) )\n",
    "    return ret\n",
    "###############################\n",
    "\n",
    "## Evaluation utilties you don't have to change\n",
    "\n",
    "def do_evaluation(examples, weights):\n",
    "    num_correct,num_total=0,0\n",
    "    for tokens,goldlabels in examples:\n",
    "        N = len(tokens); assert N==len(goldlabels)\n",
    "        predlabels = predict_seq(tokens, weights)\n",
    "        num_correct += sum(predlabels[t]==goldlabels[t] for t in range(N))\n",
    "        num_total += N\n",
    "    print \"%d/%d = %.4f accuracy\" % (num_correct, num_total, num_correct/num_total)\n",
    "    return num_correct/num_total\n",
    "\n",
    "def fancy_eval(examples, weights):\n",
    "    confusion = defaultdict(float)\n",
    "    bygold = defaultdict(lambda:{'total':0,'correct':0})\n",
    "    for tokens,goldlabels in examples:\n",
    "        predlabels = predict_seq(tokens, weights)\n",
    "        for pred,gold in zip(predlabels, goldlabels):\n",
    "            confusion[gold,pred] += 1\n",
    "            bygold[gold]['correct'] += int(pred==gold)\n",
    "            bygold[gold]['total'] += 1\n",
    "    goldaccs = {g: bygold[g]['correct']/bygold[g]['total'] for g in bygold}\n",
    "    for gold in sorted(goldaccs, key=lambda g: -goldaccs[g]):\n",
    "        print \"gold %s acc %.4f (%d/%d)\" % (gold,\n",
    "                goldaccs[gold],\n",
    "                bygold[gold]['correct'],bygold[gold]['total'],)\n",
    "\n",
    "def show_predictions(tokens, goldlabels, predlabels):\n",
    "    print \"%-20s %-4s %-4s\" % (\"word\", \"gold\", \"pred\")\n",
    "    print \"%-20s %-4s %-4s\" % (\"----\", \"----\", \"----\")\n",
    "    for w, goldy, predy in zip(tokens, goldlabels, predlabels):\n",
    "        out = \"%-20s %-4s %-4s\" % (w,goldy,predy)\n",
    "        if goldy!=predy:\n",
    "            out += \"  *** Error\"\n",
    "        print out\n",
    "\n",
    "###############################\n",
    "\n",
    "## YOUR CODE BELOW\n",
    "\n",
    "\n",
    "def train(examples, stepsize=1, numpasses=10, do_averaging=False, devdata=None):\n",
    "    \"\"\"\n",
    "    IMPLEMENT ME !\n",
    "    Train a perceptron. This is similar to the classifier perceptron training code\n",
    "    but for the structured perceptron. Examples are now pairs of token and label\n",
    "    sequences. The rest of the function arguments are the same as the arguments to\n",
    "    the training algorithm for classifier perceptron.\n",
    "    \"\"\"\n",
    "\n",
    "    weights = defaultdict(float)\n",
    "    S = defaultdict(float)\n",
    "\n",
    "    def get_averaged_weights():\n",
    "        # IMPLEMENT ME!\n",
    "        S1={}\n",
    "        for st in S:\n",
    "            S1[st] = S[st] * (1/count)\n",
    "        return dict_subtract(weights, S1)\n",
    "\n",
    "    count = 0\n",
    "    for pass_iteration in range(numpasses):\n",
    "        print \"Training iteration %d\" % pass_iteration\n",
    "        # IMPLEMENT THE INNER LOOP!\n",
    "        # Like the classifier perceptron, you may have to implement code\n",
    "        # outside of this loop as well!\n",
    "\n",
    "        for tokens,goldlabels in examples:\n",
    "            count += 1\n",
    "            predlabels = predict_seq(tokens, weights)\n",
    "            goldfeats = features_for_seq(tokens, goldlabels)\n",
    "            predfeats = features_for_seq(tokens, predlabels)\n",
    "            \n",
    "            if predlabels != goldlabels:\n",
    "                '''\n",
    "                goldfeats = {goldfeats[k]*(-stepsize) for k in goldfeats}\n",
    "                predfeats = {predfeats[k]*(stepsize) for k in predfeats}\n",
    "                weights = dict_subtract(weights, goldfeats)\n",
    "                weights = dict_subtract(weights, predfeats)\n",
    "                '''\n",
    "                g = dict_subtract(goldfeats, predfeats)\n",
    "                g1={}\n",
    "                for f in g:\n",
    "                    g1[f] = g[f] * -stepsize\n",
    "                weights = dict_subtract(weights, g1)\n",
    "                \n",
    "                for f in g:\n",
    "                    g1[f] = g[f] * (count-1) * -stepsize\n",
    "                S = dict_subtract(S, g1)\n",
    "\n",
    "        # Evaluation at the end of a training iter\n",
    "        print \"TR  RAW EVAL:\",\n",
    "        do_evaluation(examples, weights)\n",
    "        if devdata:\n",
    "            print \"DEV RAW EVAL:\",\n",
    "            do_evaluation(devdata, weights)\n",
    "        if devdata and do_averaging:\n",
    "            print \"DEV AVG EVAL:\",\n",
    "            do_evaluation(devdata, get_averaged_weights())\n",
    "\n",
    "    print \"Learned weights for %d features from %d examples\" % (len(weights), len(examples))\n",
    "\n",
    "    # NOTE different return value then classperc.py version.\n",
    "    return weights if not do_averaging else get_averaged_weights()\n",
    "\n",
    "def predict_seq(tokens, weights):\n",
    "    \"\"\"\n",
    "    IMPLEMENT ME!\n",
    "    takes tokens and weights, calls viterbi and returns the most likely\n",
    "    sequence of tags\n",
    "    \"\"\"\n",
    "    #call calc_factor_scores \n",
    "    Ascores, Bscores = calc_factor_scores(tokens, weights)\n",
    "    predlabels = viterbi(Ascores, Bscores, OUTPUT_VOCAB)\n",
    "    # once you have Ascores and Bscores, could decode with\n",
    "    # predlabels = greedy_decode(Ascores, Bscores, OUTPUT_VOCAB)\n",
    "    return predlabels\n",
    "\n",
    "def greedy_decode(Ascores, Bscores, OUTPUT_VOCAB):\n",
    "    \"\"\"Left-to-right greedy decoding.  Uses transition feature for prevtag to curtag.\"\"\"\n",
    "    N=len(Bscores)\n",
    "    if N==0: return []\n",
    "    out = [None]*N\n",
    "    out[0] = dict_argmax(Bscores[0])\n",
    "    for t in range(1,N):\n",
    "        tagscores = {tag: Bscores[t][tag] + Ascores[out[t-1], tag] for tag in OUTPUT_VOCAB}\n",
    "        besttag = dict_argmax(tagscores)\n",
    "        out[t] = besttag\n",
    "    return out\n",
    "\n",
    "def local_emission_features(t, tag, tokens):\n",
    "    \"\"\"\n",
    "    Feature vector for the B_t(y) function\n",
    "    t: an integer, index for a particular position\n",
    "    tag: a hypothesized tag to go at this position\n",
    "    tokens: the list of strings of all the word tokens in the sentence.\n",
    "    Returns a set of features.\n",
    "    \"\"\"\n",
    "    curword = tokens[t]\n",
    "    feats = {}\n",
    "    feats[\"tag=%s_biasterm\" % tag] = 1\n",
    "    feats[\"tag=%s_curword=%s\" % (tag, curword)] = 1\n",
    "    if curword[0] == '#':\n",
    "        feats[\"first_#\"] = 1\n",
    "\n",
    "    return feats\n",
    "\n",
    "def features_for_seq(tokens, labelseq):\n",
    "    \"\"\"\n",
    "    IMPLEMENT ME!\n",
    "\n",
    "    tokens: a list of tokens\n",
    "    labelseq: a list of output labels\n",
    "    The full f(x,y) function. Returns one big feature vector. This is similar\n",
    "    to features_for_label in the classifier peceptron except here we aren't\n",
    "    dealing with classification; instead, we are dealing with an entire\n",
    "    sequence of output tags.\n",
    "\n",
    "    This returns a feature vector represented as a dictionary.\n",
    "    \"\"\"\n",
    "    feat = {}\n",
    "    #emission probabilities\n",
    "    for i,token in enumerate(tokens):\n",
    "        fb = local_emission_features(0, labelseq[i], [token])\n",
    "        for f in fb:\n",
    "            if f in feat:\n",
    "                feat[f] += fb[f]\n",
    "            else:\n",
    "                feat[f] = fb[f]\n",
    "    \n",
    "    #transition probabilities\n",
    "    last_label = None\n",
    "    for label in labelseq:\n",
    "        if last_label is not None:\n",
    "            trans = \"lasttag=%s_curtag=%s\" % (last_label, label)\n",
    "            if trans in feat:\n",
    "                feat[trans] += 1\n",
    "            else:\n",
    "                feat[trans] = 1\n",
    "                \n",
    "            \n",
    "        last_label = label\n",
    "        \n",
    "    return feat\n",
    "    \n",
    "def calc_factor_scores(tokens, weights):\n",
    "    \"\"\"\n",
    "    IMPLEMENT ME!\n",
    "\n",
    "    tokens: a list of tokens\n",
    "    weights: perceptron weights (dict)\n",
    "\n",
    "    returns a pair of two things:\n",
    "    Ascores which is a dictionary that maps tag pairs to weights\n",
    "    Bscores which is a list of dictionaries of tagscores per token\n",
    "    \"\"\"\n",
    "    N = len(tokens)\n",
    "    # MODIFY THE FOLLOWING LINE\n",
    "    Ascores = { (tag1,tag2): weights[\"lasttag=%s_curtag=%s\" % (tag1, tag2)] for tag1 in OUTPUT_VOCAB for tag2 in OUTPUT_VOCAB }\n",
    "    \n",
    "    Bscores = []\n",
    "    for t in range(N):\n",
    "        # IMPLEMENT THE INNER LOOP\n",
    "        dt= {}\n",
    "        for tag in OUTPUT_VOCAB:\n",
    "            feats = local_emission_features(t, tag, tokens)\n",
    "            f = \"tag=%s_curword=%s\" % (tag, tokens[t]) \n",
    "            fbias =   \"tag=%s_biasterm\" % tag                              \n",
    "            dt[tag] = (feats[f]* weights.get(f,0)) + (feats[fbias] *weights[fbias])\n",
    "\n",
    "            \n",
    "        Bscores.append(dt)\n",
    "    assert len(Bscores) == N\n",
    "    return Ascores, Bscores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print local_emission_features(1,'V', ['I','love','cats'])\n",
    "    print local_emission_features(0,'A', ['Happy','president','Abe'])\n",
    "    print features_for_seq(['Happy','president','Abe', 'Lincoln'], ['A','N','N','N'])\n",
    "    A, B = calc_factor_scores(['I','love'], defaultdict(float))\n",
    "    print A\n",
    "    print B\n",
    "    print predict_seq(['I','love'], defaultdict(float))\n",
    "    tweet_lst = read_tagging_file('oct27.train.txt')\n",
    "    devdata = read_tagging_file('oct27.dev.txt')\n",
    "    weights_end = train(tweet_lst, stepsize=1, numpasses=10, do_averaging=True, devdata=devdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold O acc 0.9489 (316/333)\n",
      "gold , acc 0.9480 (474/500)\n",
      "gold D acc 0.9295 (290/312)\n",
      "gold & acc 0.9231 (84/91)\n",
      "gold P acc 0.9205 (405/440)\n",
      "gold V acc 0.8322 (625/751)\n",
      "gold L acc 0.7538 (49/65)\n",
      "gold T acc 0.7500 (27/36)\n",
      "gold N acc 0.7000 (462/660)\n",
      "gold R acc 0.6555 (137/209)\n",
      "gold A acc 0.5900 (141/239)\n",
      "gold E acc 0.5577 (29/52)\n",
      "gold ! acc 0.5253 (52/99)\n",
      "gold @ acc 0.4280 (104/243)\n",
      "gold $ acc 0.3605 (31/86)\n",
      "gold U acc 0.2747 (25/91)\n",
      "gold ^ acc 0.2379 (74/311)\n",
      "gold G acc 0.2000 (13/65)\n",
      "gold # acc 0.0577 (3/52)\n",
      "gold S acc 0.0000 (0/5)\n",
      "gold X acc 0.0000 (0/4)\n",
      "gold Z acc 0.0000 (0/9)\n",
      "gold ~ acc 0.0000 (0/170)\n"
     ]
    }
   ],
   "source": [
    "fancy_eval(devdata, weights_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                 gold pred\n",
      "----                 ---- ----\n",
      "RT                   ~    A     *** Error\n",
      "@TheRealQuailman     @    N     *** Error\n",
      ":                    ~    ,     *** Error\n",
      "Currently            R    A     *** Error\n",
      "laughing             V    N     *** Error\n",
      "at                   P    P   \n",
      "Laker                ^    ^   \n",
      "haters               N    N   \n",
      ".                    ,    ,   \n"
     ]
    }
   ],
   "source": [
    "predlabels = predict_seq(devdata[1][0], weights_end)\n",
    "show_predictions(devdata[1][0], devdata[1][1], predlabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
